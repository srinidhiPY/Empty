
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
	

<head>
	<title>Chetan Srinidhi</title>
	
        <meta charset="utf-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
	
        <meta name="keywords" content="Chetan Srinidhi"> 
        <meta name="description" content="Chetan Srinidhi's home page">
	
	<link rel="shortcut icon" href="myIcon.ico">
	
	<!-- CSS  -->
	<link rel="stylesheet" href="jemdoc.css" type="text/css">
</head>
	
	
<body>
<!-- Initial Photo and Contact Details ***************** -->
<table>
    <tbody>
        <tr>
            <td width="670">
                <div id="toptitle">                 
                    <h1>Chetan L Srinidhi</h1><h1>
                </h1></div>

                <h3>Postdoctoral Research Fellow</h3>
                <p>
		    Sunnybrook Research Institute, <br>
                    Medical Biophysics, University of Toronto,<br>
                    Toronto, Canada - M4N3M5.<br>
                    <br>
                </p>
                <p> 
                    <strong>Personal:&nbsp;</strong>&nbsp; <a href="https://scholar.google.co.in/citations?user=H6634DcAAAAJ&hl=en" target="_blank">Google Scholar</a>, <a href="https://github.com/srinidhiPY" target="_blank">Github</a>, <a href="https://twitter.com/ChetanSrinidhi" target="_blank">Twitter</a>, <a href="https://www.linkedin.com/in/chetan-srinidhi-a44437178/" target="_blank">Linkedin</a> <br/><strong>Email:&nbsp;</strong>chetan.srinidhi@utoronto.ca, srinidhipy@gmail.com<br/><strong>Research Group:&nbsp;</strong>&nbsp;<a href="http://martellab.com/" target="_blank">Martel Lab</a>
            <td>
                <img src="./images/Chetan_Srinidhi.png" border="10" width="250"><br>
            </td>
        </tr><tr>
    </tr></tbody>
</table>
	

<!-- Short Bio and Research Interests ***************** -->
<h2>Bio</h2>
<p align="justify">
I am a Postdoctoral Researcher at <a href="https://sunnybrook.ca/research/">Sunnybrook Research Institute</a>, <a href="https://medbio.utoronto.ca/medical-biophysics">Department of Medical Biophysics</a>, <a href="https://www.utoronto.ca"><b>University of Toronto</b></a>, supervised by <a href="https://medbio.utoronto.ca/faculty/martel"><b>Prof. Anne Martel</b></a>. I work in computational histopathology, encompassing machine learning and artificial intelligence based methods to estimate cancer recurrence, survival/patient outcomes from histopathology whole-slide images.</p>
<p align="justify">
Previously, I completed my Ph.D. at Department of Electronics and Communication Engineering</a>, <a href="https://www.nitk.ac.in/">National Institute of Technology Karnataka (NITK), Surathkal</a>, India, under the supervision of <a href="http://ece.nitk.ac.in/faculty/aparna-p">Dr. Aparna P</a> and <a href="https://scholar.google.co.in/citations?hl=en&user=7YrGeNoAAAAJ&view_op=list_works&sortby=pubdate">Dr. Jeny Rajan</a>. My thesis was focused on "Pattern Recognition and Machine Learning Framework for Automated Analysis of Retinal Images". During my PhD, I've spent two wonderful semesters as a Research Intern with <a href="https://www.iiit.ac.in/people/faculty/jsivaswamy/">Prof. Jayanthi Sivaswamy</a> at <a href="https://www.iiit.ac.in/">International Institute of Information Technology (IIIT) Hyderabad</a> on retinal image analysis. Before joining NITK, I obtained my Master's degree from <a href="https://www.rvce.edu.in/">R. V. College of Engineering</a>, Bangalore, and Bachelor's degree from Visvesvaraya Technological University, Belgaum, India.  
</p>

<h2>Research Interests</h2>
<p align="justify">
<b>Medical Image Analysis</b>, <b>Computational Pathology</b>, <b>Machine Learning</b>,  <b>Computer Vision.</b></p>

<p align="justify">
  My research focuses on the intersection of computational pathology and artificial intelligence for improving cancer diagnosis and patient outcomes.  
  Specifically, I am interested in developing machine learning methods for Digital Pathology with a specific focus on Self-supervised, Semi-supervised and Weakly-supervised learning. 
</p>



<!-- Publications ***************** -->
<h2>Publications</h2>
<table style="width:100%">  
	
    <tr>
        <td width="500">
        <img src="images/Hadcl_figure.png" width="400px" style="box-shadow: 4px 4px 8px #888">
        </td> <td </td>            	
   <p><a 
      <heading><b>Improving Self-supervised Learning with Hardness-aware Dynamic Curriculum Learning: An Application to Digital Pathology</b></heading></a><br><br>
      <b>Chetan L Srinidhi</b>, Anne L Martel<br>
      International Conference on Computer Vision (<b>ICCV</b>), 2021, CDpath Workshop (Oral)
      </p>
        <p>[<a href="https://arxiv.org/pdf/2108.07183.pdf" target="_blank">Paper</a>]
	   [<a href="https://github.com/srinidhiPY/ICCV-CDPATH2021-ID-8" target="_blank">Code with pretrained models</a>]
           [<a href="bibtex.txt">Bibtex</a>] </p>
        <div style="height:150px;width:510px;overflow:auto;background-color:#F0E68C;scrollbar-base-color:gold;font-face:Arial;font-size:12px;padding:10px;overflow:auto;border:1px solid #abf;">
Self-supervised learning (SSL) has recently shown tremendous potential to learn generic visual representations useful for many image analysis tasks. Despite their notable success, the existing SSL methods fail to generalize to downstream tasks when the number of labeled training instances is small or if the domain shift between the transfer domains is significant. 
In this paper, we attempt to improve self-supervised pretrained representations through the lens of curriculum learning by proposing a hardness-aware dynamic curriculum learning (HaDCL) approach. To improve the robustness and generalizability of SSL, we dynamically leverage progressive harder examples via easy-to-hard and hard-to-very-hard samples during mini-batch downstream fine-tuning. We discover that by progressive stage-wise curriculum learning, the pretrained representations are significantly enhanced and adaptable to both in-domain and out-of-domain distribution data.
We performed extensive validation on three histology benchmark datasets on both patch-wise and slide-level classification problems. Our curriculum based fine-tuning yields a significant improvement over standard fine- tuning, with a minimum improvement in area-under-the-curve (AUC) score of 1.7% and 2.2% on in-domain and out-of-domain distribution data, respectively. Further, we empirically show that our approach is more generic and adaptable to any SSL methods and does not impose any additional overhead complexity. Besides, we also outline the role of patch-based versus slide-based curriculum learning in histopathology to provide practical insights into the success of curriculum based fine-tuning of SSL methods.		
</div>
        </td>
    </tr>
    <tr></tr>
    <tr></tr>
    <tr></tr> <tr></tr> <tr></tr> <tr></tr> <tr></tr> <tr></tr> <tr></tr>

			
    <tr>
        <td width="500">
        <img src="images/SSL_CR.png" width="400px" style="box-shadow: 4px 4px 8px #888">
        </td>              
      <p><a 
      <heading><b>Self-supervised driven consistency training for annotation efficient histopathology image analysis</b></heading></a><br><br>
      <b>Chetan L Srinidhi</b>, Seung Wook Kim, Fu-Der Chen, Anne L Martel<br>
      in Press, Medical Image Analysis <b>(MedIA)</b>, 2021 <br> <b><font color="#A52A2A">(Top Journal â€“ IF: 8.545)</font></b> 
      </p>				
        <p>[<a href="https://arxiv.org/pdf/2102.03897.pdf" target="_blank">Paper</a>]
	   [<a href="https://github.com/srinidhiPY/SSL_CR_Histo" target="_blank">Code with pretrained models</a>]
           [<a href="bibtex.txt">Bibtex</a>] </p>
        <div style="height:150px;width:510px;overflow:auto;background-color:#F0E68C;scrollbar-base-color:gold;font-face:Arial;font-size:12px;padding:10px;overflow:auto;border:1px solid #abf;">
Training a neural network with a large labeled dataset is still a dominant paradigm in computational histopathology. However, obtaining such exhaustive manual annotations is often expensive, laborious, and prone to inter and intra-observer variability. While recent self-supervised and semi-supervised methods can alleviate this need by learning unsupervised feature representations, they still struggle to generalize well to downstream tasks when the number of labeled instances is small. 
In this work, we overcome this challenge by leveraging both task-agnostic and task-specific unlabeled data based on two novel strategies: i) a self-supervised pretext task that harnesses the underlying multi-resolution contextual cues in histology whole-slide images to learn a powerful supervisory signal for unsupervised representation learning; ii) a new teacher-student semi-supervised consistency paradigm that learns to effectively transfer the pretrained representations to downstream tasks based on prediction consistency with the task-specific unlabeled data.
We carry out extensive validation experiments on three histopathology benchmark datasets across two classification and one regression based tasks, i.e., tumor metastasis detection, tissue type classification, and tumor cellularity quantification. Under limited label data, the proposed method yields tangible improvements, which is close or even outperforming other state-of-the-art self-supervised and supervised baselines. Furthermore, we empirically show that the idea of bootstrapping the self-supervised pretrained features is an effective way to improve the task-specific semi-supervised learning on standard benchmarks.
</div>
        </td>
    </tr>
    <tr></tr>
    <tr></tr>
    <tr></tr> <tr></tr> <tr></tr> <tr></tr> <tr></tr> <tr></tr> <tr></tr>
	
	
    <tr>
        <td width="500">
        <img src="images/Main_review_Figure.png" width="400px" style="box-shadow: 4px 4px 8px #888">
        </td>              
        <td align="justify"><b>Chetan L Srinidhi</b>, Ozan Ciga, Anne L Martel. <b>"Deep Neural Network Models for Computational Histopathology: A Survey."</b> <em>Medical Image Analysis <b>(MedIA)</b>, 2020. <b><font color="#A52A2A">(Top Journal â€“ IF: 8.545)</b> <p></p> <p><b>Most downloaded article in MedIA</font></b></p> </em>
        <p></p>
        <p>[<a href="https://www.sciencedirect.com/science/article/abs/pii/S1361841520301778" target="_blank">Paper</a>]
           [<a href="bibtex.txt">Bibtex</a>] </p>
        <div style="height:150px;width:510px;overflow:auto;background-color:#F0E68C;scrollbar-base-color:gold;font-face:Arial;font-size:12px;padding:10px;overflow:auto;border:1px solid #abf;">
In this paper, we present a comprehensive review of state-of-the-art deep learning approaches that have been used in the context of histopathological image analysis. From the survey of over 130 papers, we review the fieldâ€™s progress based on the methodological
aspect of different machine learning strategies such as supervised, weakly supervised, unsupervised, transfer learning
and various other sub-variants of these methods. We also provide an overview of deep learning based survival models that are applicable for disease-specific prognosis tasks. Finally, we summarize several existing open datasets and
highlight critical challenges and limitations with current deep learning approaches, along with possible avenues for future research.
</div>
        </td>
    </tr>
    <tr></tr>
    <tr></tr>
    <tr></tr> <tr></tr> <tr></tr> <tr></tr> <tr></tr> <tr></tr> <tr></tr>
  	    
    <tr>
        <td width="500">
        <img src="images/TIP_A-V.png" width="400px" style="box-shadow: 4px 4px 8px #888">
        </td>               
        <td align="justify"><b>Chetan L Srinidhi</b>, P Aparna, and Jeny Rajan. <b>"Automated Method for Retinal Artery/Vein Separation via Graph Search Metaheuristic Approach."</b> <em>IEEE Transactions on Image Processing <b>(TIP)</b>, 2019. <b><font color="#A52A2A">(Top Journal â€“ IF: 10.856)</font></b></em>
        <p></p>
        <p>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598955" target="_blank">Paper</a>]
        [<a href="https://ieeexplore.ieee.org/ielx7/83/4358840/8598955/Supp_Material_TIP2889534.pdf?tp=&arnumber=8598955" target="_blank">Supplementary Material</a>]    
        <!--[<a href="https://github.com/yulequan/HeartSeg" target="_blank">code1</a>]--> [<a href="bibtex.txt">Bibtex</a>] </p>
        <div style="height:50px;width:510px;overflow:auto;background-color:#F0E68C;scrollbar-base-color:gold;font-face:Arial;font-size:12px;padding:10px;overflow:auto;border:1px solid #abf;">
		In this paper, we present a novel graph search metaheuristic approach for automatic separation of arteries/veins (A/V) from color fundus images. 
		Our method exploits local information to disentangle the complex vascular tree into multiple subtrees, and global information to label these vessel subtrees into arteries and veins. 
		Given a binary vessel map, a graph representation of the vascular network is constructed representing the topological and spatial connectivity of the vascular structures. 
		Based on the anatomical uniqueness at vessel crossing and branching points, the vascular tree is split into multiple subtrees containing arteries and veins. 
		Finally, the identified vessel subtrees are labeled with A/V based on a set of handcrafted features trained with random forest classifier. 
		The proposed method has been tested on four different publicly available retinal datasets with an average accuracy of 94.7%, 93.2%, 96.8% and 90.2% across AV-DRIVE, CT-DRIVE. INSPIRE-AVR and WIDE datasets, respectively. 
		These results demonstrate the superiority of our proposed approach in outperforming state-of-the- art methods for A/V separation.			
</div>
        </td>
    </tr>
    <tr></tr>
    <tr></tr>
    <tr></tr>

	    
     <tr>
        <td width="500">
        <img src="images/BSPC.png" width="400px" style="box-shadow: 4px 4px 8px #888">
        </td>               
        <td align="justify"><b>Chetan L Srinidhi</b>, P Aparna, and Jeny Rajan. <b>"A Visual Attention Guided Unsupervised Feature Learning for Robust Vessel Delineation in Retinal Images."</b> <em>Biomedical Signal Processing and Control, 2018. <b><font color="#A52A2A">(IF: 3.88)</font></b></em>
        <p></p>
        <p>[<a href="https://www.sciencedirect.com/science/article/pii/S1746809418301010" target="_blank">Paper</a>]
        <!--[<a href="https://github.com/yulequan/HeartSeg" target="_blank">code1</a>]-->  [<a href="bibtex.txt">Bibtex</a>]</p>
        <div style="height:200px;width:510px;overflow:auto;background-color:#F0E68C;scrollbar-base-color:gold;font-face:Arial;font-size:12px;padding:10px;overflow:auto;border:1px solid #abf;">
		We propose a novel visual attention guided unsupervised feature learning (VA-UFL) approach to automatically learn the most discriminative features for segmenting vessels in retinal images. 
		Our VA-UFL approach captures both the knowledge of visual attention mechanism and multi-scale contextual information to selectively visualize the most relevant part of the structure in a given local patch. 
		This allows us to encode a rich hierarchical information into unsupervised filtering learning to generate a set of most discriminative features that aid in the accurate segmentation of vessels, even in the presence of cluttered background.
		Our proposed method is validated on the five publicly available retinal datasets: DRIVE, STARE, CHASE_DB1, IOSTAR and RC-SLO. 
		The experimental results show that the proposed approach significantly outperformed the state-of-the-art methods in terms of sensitivity, accuracy and area under the receiver operating characteristic curve across all five datasets. 
		Specifically, the method achieved an average sensitivity greater than 0.82, which is 7% higher compared to all existing approaches validated on DRIVE, CHASE_DB1, IOSTAR and RC-SLO datasets, and outperformed even second-human observer. 
		The method is shown to be robust to segmentation of thin vessels, strong central vessel reflex, complex crossover structures and fares well on abnormal cases.
</div>
        </td>
    </tr>
    <tr></tr>
    <tr></tr>
    <tr></tr>  

	    
    <tr>
        <td width="500">
        <img src="images/ISBI17.png" width="400px" style="box-shadow: 4px 4px 8px #888">
        </td>               
        <td align="justify"><b>Chetan L Srinidhi</b>, Priyadarshi Rath, Jayanthi Sivaswamy. <b>"A Vessel Keypoint Detector for Junction classification."</b> <em>IEEE International Symposium on Biomedical Imaging <b>(ISBI)</b>, 2017. <b><font color="#A52A2A">(Oral Presentation, Acceptance Rate: ~19%)</font></b></b></em>
        <p></p>
        <p>[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950657" target="_blank">Paper</a>]   
        [<a href="bibtex.txt">Bibtex</a>]</p>
        <div style="height:60px;width:510px;overflow:auto;background-color:#F0E68C;scrollbar-base-color:gold;font-face:Arial;font-size:12px;padding:10px;overflow:auto;border:1px solid #abf;">
		In this paper, we propose a novel Vessel Keypoint Detector (VKD) which is derived from the projection of log-polar transformed binary patches around vessel points. 
		VKD is used to design a two stage solution for junction detection and classification. In the first stage, the keypoints detected using VKD are refined using curvature orientation information to extract candidate junctions. 
		True junctions from these candidates are identified in a supervised manner using a Random Forest classifier. 
		In the next stage, a novel combination of local orientation and shape based features is extracted from the junction points and classified using a second Random Forest classifier. 
		Evaluation results on five datasets show that the designed system is robust to changes in resolution and other variations across datasets, with average values of accuracy/sensitivity/specificity for junction detection being 0.78/0.79/0.75 and for junction classification being 0.87/0.85/0.88. 
		Our system outperforms the state of the art method by at least 11%, on the DRIVE and IOSTAR datasets. These results demonstrate the effectiveness of VKD for vessel analysis.			
</div>
        </td>
    </tr>
    <tr></tr>      <tr></tr>
    <tr></tr> <tr></tr> <tr></tr> <tr></tr>     <tr></tr> <tr></tr> <tr></tr> <tr></tr>  
   
    <tr>
    	<td></td>
        <td>
        <!-- <ul> -->
        <!-- <li> -->
        <p align="justify">
            <b>Chetan L Srinidhi</b>, P Aparna, and Jeny Rajan. <b>"Recent Advancements in Retinal Vessel Segmentation."</b> <em>Journal of Medical Systems, 2017. <b><font color="#A52A2A">(IF: 4.460)</font></b></em>
        </p>
        <p>
            [<a href="https://link.springer.com/content/pdf/10.1007%2Fs10916-017-0719-2.pdf" target="_blank">Paper</a>]   [<a href="bibtex.txt">Bibtex</a>] 
        </p>
        <!-- </li> -->
        <!-- </ul> -->
    	<div style="height:200px;width:510px;overflow:auto;background-color:#F0E68C;scrollbar-base-color:gold;font-face:Arial;font-size:12px;padding:10px;overflow:auto;border:1px solid #abf;">
		<p align="justify">		
		In this paper, we carry out a systematic review of the most recent advancements in retinal vessel segmentation methods published in last five years. The objectives of this study are as follows: 
		first, we discuss the most crucial preprocessing steps that are involved in accurate segmentation of vessels. Second, we review most recent state-of-the-art retinal vessel segmentation techniques which are classified into different categories based on their main principle. 
		Third, we quantitatively analyse these methods in terms of its sensitivity, specificity, accuracy, area under the curve and discuss newly introduced performance metrics in current literature. Fourth, we discuss the advantages and limitations of the existing segmentation techniques. 
		Finally, we provide an insight into active problems and possible future directions towards building successful computer-aided diagnostic system.
</p>
		</div>
	    </td>
    </tr>

</table>

<h2>Invited Talks</h2>

<li>
<b>Learning with Reduced Human Supervision in Digital Pathology</b> at <a href="https://www.pathai.com" target="_blank">PathAI, Boston, Research Speaker Series</a>, USA, Aug, 2021.	
</li>
<li>
<b>Learning with Self-Supervision: An Application to Digital Pathology</b> at <a href="https://malmic.ca/pathology-forum-april-16-2021/" target="_blank">Machine Learning in Medical Imaging Consortium (MaLMIC), Ontario</a>, Canada, April, 2021.	
</li>
<li>
<b>Self-supervised driven consistency training for annotation efficient histopathology image analysis</b> in the research group seminar of <a href="https://www.gwtaylor.ca/#group" target="_blank">Prof. Graham Taylor</a>, University of Guelph, Canada, Feb, 2021.	
</li>
<li>
<b>Self-supervised driven consistency training for annotation efficient histopathology image analysis</b> at <a href="https://vectorinstitute.ai" target="_blank">Vector Institute Research Symposium</a>, Toronto, Canada, Feb, 2021.	
</li>

<h2>Professional Services</h2>
<li>
    <b>Journal Reviews:</b> 
     <ul style="list-style-type:none;">	
     <li>Medical Image Analysis, Elsevier (<b>MedIA</b>)</li>
     <li>IEEE Transactions on Medical Imaging (<b>TMI</b>)</li>
     <li>IEEE Journal of Biomedical and Health Informatics (<b>JBHI</b>)</li> 
     <li>IEEE Transactions on Industrial Informatics (<b>TII</b>)</li> 
     <li>IEEE <b>Access</b></li> 
     <li>etc.</li></ul>
    <p style="margin-top:3px"></p> </li>
<li>
    <b>Conference Reviews:</b> 
     <ul style="list-style-type:none;">	
     <li>Medical Image Computing and Computer Assisted Intervention (<b>MICCAI</b>), 2020, 2021</li>
    <p style="margin-top:3px"></p>	
</li>


<div id="footer">
    <div id="footer-text"></div>
</div>
    <p><center>
        
    <br>
        Last updated: 30/Sept/2021
     
      </center></p>


</div>
</body></html>
